{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import json\n",
    "import spacy\n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyemd import emd\n",
    "from gensim.similarities import WmdSimilarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "from gensim.models import KeyedVectors\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# you need to install gensim and download the glove.6b.200d.txt online\n",
    "\n",
    "# inputs are two sentence, s1 is the question, s2 is the sentence from the text\n",
    "# the output is the distance between two sentences. When the number is smaller, the two sentences are more similar.\n",
    "def calculate_distance(s1, s2):\n",
    "    glove_input_file = 'glove.6B.200d.txt'\n",
    "    word2vec_output_file = 'glove.6B.200d.txt.word2vec'\n",
    "    glove2word2vec(glove_input_file, word2vec_output_file)\n",
    "\n",
    "    # remove stopwords\n",
    "    s1 = remove_stopwords(s1)\n",
    "    s2 = remove_stopwords(s2)\n",
    "\n",
    "    # load the Stanford GloVe model\n",
    "    filename = 'glove.6B.200d.txt.word2vec'\n",
    "    model = KeyedVectors.load_word2vec_format(filename, binary=False)\n",
    "\n",
    "    # calculate distance between two sentences using WMD algorithm\n",
    "    distance = model.wmdistance(s1, s2)\n",
    "\n",
    "    return distance\n",
    "\n",
    "\n",
    "def remove_stopwords(s):\n",
    "    # remove stopwords\n",
    "    stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "\n",
    "    word_tokens = word_tokenize(s)\n",
    "    filtered_sentence = [w.strip() for w in word_tokens if w not in stopwords]\n",
    "    text = \" \".join(str(x) for x in filtered_sentence)\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph = \"In November 2006, the Victorian Legislative Council elections were held under a new multi-member proportional representation system. The State of Victoria was divided into eight electorates with each electorate represented by five representatives elected by Single Transferable Vote. The total number of upper house members was reduced from 44 to 40 and their term of office is now the same as the lower house membersâ€”four years. Elections for the Victorian Parliament are now fixed and occur in November every four years. Prior to the 2006 election, the Legislative Council consisted of 44 members elected to eight-year terms from 22 two-member electorates.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_sentences(paragraph): \n",
    "    sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "    sentences = sent_detector.tokenize(paragraph.strip())\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "sen = parse_sentences(paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "# answer_sen = ''\n",
    "# max_score = 0\n",
    "# question = 'How many campuses does the University of California have?'\n",
    "# for sen in parse_sentences(paragraph):\n",
    "#     new_score = calculate_distance(sen, question)\n",
    "#     if new_score > maxscore:\n",
    "#         maxscore = new_score\n",
    "#         answer_sen = sen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = \"How many metropolitan areas does Southern California's population encompass?\"\n",
    "sentence = \"Southern California's population encompasses seven metropolitan areas, or MSAs: the Los Angeles metropolitan area, consisting of Los Angeles and Orange counties; the Inland Empire, consisting of Riverside and San Bernardino counties\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_entity(sentences):\n",
    "    entity = []\n",
    "    doc = nlp(sentences)\n",
    "    for ent in doc.ents:\n",
    "        entity.append( (ent.text,ent.label_))\n",
    "    return entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find cardinal entity\n",
    "#input: sentence\n",
    "def locate_cardinal(sentence):\n",
    "    cardinal = []\n",
    "    for en in get_entity(sentence):\n",
    "        if en[1] == 'CARDINAL':\n",
    "            cardinal.append(en[0])\n",
    "    return cardinal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lemma(word):\n",
    "    token = nlp(word)\n",
    "    lemma = token[0].lemma_\n",
    "    return lemma\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_word_distance(lemma, card, sentence):\n",
    "    sentence_token = nlp(sentence)\n",
    "    idx = 0\n",
    "    obj_idx = 0\n",
    "    card_idx = 0\n",
    "    for s in sentence_token:\n",
    "        #print(s.text)\n",
    "        if s.lemma_ == lemma:\n",
    "            obj_idx = idx\n",
    "        if s.text == card:\n",
    "            card_idx = idx\n",
    "        idx += 1\n",
    "    if card_idx - obj_idx == 0:\n",
    "        return 9999\n",
    "    else:\n",
    "        return abs(card_idx - obj_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_how(question, sentence):\n",
    "    noun = get_noun(question)\n",
    "    noun_lemma = get_lemma(noun)\n",
    "    card_list = locate_cardinal(sentence)\n",
    "    min_dist = 9999\n",
    "    answer = ''\n",
    "    for card in card_list:\n",
    "        #print(noun_lemma)\n",
    "        dist = calculate_word_distance(noun_lemma, card, sentence)\n",
    "        if dist < min_dist:\n",
    "            mind_dist = dist\n",
    "            answer = card\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_noun(sentence):\n",
    "    doc = nlp(sentence)\n",
    "    for token in doc:\n",
    "        if token.pos_ == 'NOUN':\n",
    "            return token.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'seven'"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_how(q, sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
